<div align="center">

<img src="assets/shimmytok-logo.png" alt="shimmytok - Pure Rust tokenizer for GGUF models" width="400">

### Pure Rust tokenizer for GGUF models
**100% llama.cpp compatible â€¢ zero C++ â€¢ just works**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Crates.io](https://img.shields.io/crates/v/shimmytok.svg)](https://crates.io/crates/shimmytok)
[![Rust](https://img.shields.io/badge/rust-stable-brightgreen.svg)](https://rustup.rs/)
[![ğŸ’ Sponsor](https://img.shields.io/badge/ğŸ’_Sponsor-ea4aaa?style=flat&logo=github&logoColor=white)](https://github.com/sponsors/Michael-A-Kuykendall)

</div>

---

**shimmytok is free forever.** MIT licensed, no strings attached.

ğŸ’ **If shimmytok helps you, consider [sponsoring](https://github.com/sponsors/Michael-A-Kuykendall).**

---

## âœ¨ What is shimmytok?

shimmytok is a **pure Rust tokenizer library** that reads tokenizers directly from GGUF model files. No Python, no C++, no separate tokenizer files â€” just point it at your `.gguf` and go.

### Why does this matter?

When you download a GGUF model, the tokenizer is embedded inside. Most Rust projects either:
- Bind to C++ (llama.cpp FFI) â€” adds build complexity
- Use separate tokenizer files â€” requires extra downloads
- Roll their own â€” risk of incompatibility

**shimmytok extracts and runs the tokenizer directly from your GGUF file**, producing identical output to llama.cpp.

## ğŸ¯ v0.7.0 Highlights

This release achieves **full llama.cpp tokenizer parity**:

- âœ… **6 tokenizer algorithms** â€” SPM, BPE, WPM, UGM, RWKV, PLaMo-2
- âœ… **41 BPE pre-tokenization patterns** â€” GPT-2, Llama-3, Qwen, DeepSeek, and more
- âœ… **10/10 vocab models validated** â€” Exact token match against `llama-tokenize`
- âœ… **~2,800 lines of Rust** â€” Focused, auditable, no bloat

## Features

- ğŸ¦€ **Pure Rust** â€” No C++ dependencies, compiles anywhere
- ğŸ“¦ **Load from GGUF** â€” Tokenizer embedded in model file
- âœ… **Validated** â€” Every algorithm tested against llama.cpp
- âš¡ **Fast** â€” Batch encoding with Rayon parallelism
- ğŸŒŠ **Streaming** â€” Token-by-token decoding for LLM output
- ğŸ”’ **Safe** â€” Zero unsafe code in critical paths

## Installation

```toml
[dependencies]
shimmytok = "0.7"
```

## Quick Start

```rust
use shimmytok::Tokenizer;

// Load tokenizer from any GGUF model
let tokenizer = Tokenizer::from_gguf_file("llama-3.gguf")?;

// Encode text to tokens
let tokens = tokenizer.encode("Hello, world!", true)?;
println!("Tokens: {:?}", tokens);

// Decode back to text
let text = tokenizer.decode(&tokens, true)?;
println!("Text: {}", text);

// Stream tokens one at a time (for LLM generation)
for token_id in tokens {
    print!("{}", tokenizer.decode_single(token_id)?);
}
```

## Validated Models

All models produce **exact token match** with `llama-tokenize`:

| Model | Tokenizer | Status |
|-------|-----------|--------|
| llama-spm | SentencePiece | âœ… Match |
| gpt-2 | BPE | âœ… Match |
| qwen2 | BPE | âœ… Match |
| starcoder | BPE | âœ… Match |
| deepseek-coder | BPE | âœ… Match |
| deepseek-llm | BPE | âœ… Match |
| falcon | BPE | âœ… Match |
| command-r | BPE | âœ… Match |
| refact | BPE | âœ… Match |
| bert-bge | WordPiece | âœ… Match |

## Tokenizer Algorithms

shimmytok implements all tokenizer types from llama.cpp:

| Type | Algorithm | Models |
|------|-----------|--------|
| **SPM** | SentencePiece with resegment | LLaMA, Mistral, Gemma |
| **BPE** | Byte-Pair Encoding + regex pre-tokenization | GPT-2, Qwen, StarCoder, DeepSeek |
| **WPM** | WordPiece (BERT-style) | BERT, BGE embeddings |
| **UGM** | Unigram (Viterbi DP) | T5, mT5 |
| **RWKV** | Trie-based greedy | RWKV World |
| **PLaMo-2** | Table-driven reverse DP | PLaMo-2 |

### BPE Pre-tokenization Patterns

shimmytok supports **41 different regex patterns** for BPE pre-tokenization, covering:

- GPT-2/GPT-3/GPT-4 style
- Llama-3 style  
- Qwen/Qwen2 style
- DeepSeek (coder + LLM variants)
- StarCoder/StarCoder2
- Falcon, Command-R, DBRX
- And many more...

## API Reference

### Core Methods

```rust
// Load from GGUF file
let tokenizer = Tokenizer::from_gguf_file("model.gguf")?;

// Encode text â†’ tokens
let tokens = tokenizer.encode("Hello", true)?;  // true = add BOS/EOS

// Decode tokens â†’ text  
let text = tokenizer.decode(&tokens, true)?;    // true = skip special tokens

// Streaming decode (for LLM generation)
let piece = tokenizer.decode_single(token_id)?;
```

### Metadata

```rust
tokenizer.vocab_size()    // â†’ usize
tokenizer.bos_token()     // â†’ Option<TokenId>  
tokenizer.eos_token()     // â†’ Option<TokenId>
tokenizer.model_type()    // â†’ &str ("llama", "gpt2", etc.)
tokenizer.pre_type()      // â†’ &str (pre-tokenization pattern)
```

### Batch & Advanced

```rust
// Parallel batch encoding
let batch = tokenizer.encode_batch(&["text1", "text2"], true)?;

// Token introspection
tokenizer.token_to_piece(token_id)?  // â†’ Vec<u8>
tokenizer.token_type(token_id)       // â†’ Option<TokenType>
tokenizer.is_special_token(token_id) // â†’ bool
```

## Use Cases

- **LLM Inference Engines** â€” Pure Rust inference without C++ bindings
- **WASM Applications** â€” Run tokenization in the browser
- **Embedded Systems** â€” No C++ toolchain required
- **CLI Tools** â€” Inspect and debug GGUF tokenizers
- **Research** â€” Understand tokenization without black boxes

## Performance

shimmytok prioritizes **correctness over speed**, but it's still fast:

- Vocabulary caching (HashMap lookups)
- Rayon-parallel batch encoding
- Efficient trie structures for UGM/RWKV

For most use cases, tokenization is not the bottleneck â€” inference is.

## Links

- **ğŸ“– [CHANGELOG](CHANGELOG.md)** â€” Version history  
- **ğŸ—ºï¸ [ROADMAP](ROADMAP.md)** â€” Future plans
- **ğŸ¤ [CONTRIBUTING](CONTRIBUTING.md)** â€” How to contribute
- **ğŸ”’ [SECURITY](SECURITY.md)** â€” Vulnerability reporting
- **ğŸ“š [docs.rs](https://docs.rs/shimmytok)** â€” API documentation

## Related Projects

- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** â€” Reference C++ implementation
- **[GGUF spec](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)** â€” File format documentation

## License

MIT License â€” free forever, no strings attached.

---

**Maintainer**: Michael A. Kuykendall  
**Mission**: Pure Rust tokenization for the LLM ecosystem
