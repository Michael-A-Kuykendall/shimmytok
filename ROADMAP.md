# üó∫Ô∏è shimmytok Roadmap

**Current Version**: v0.7.0  
**Status**: Production Ready  
**Primary Use Case**: GGUF tokenization for libshimmy integration

---

## üéØ Project Mission

shimmytok is a **pure Rust tokenizer library** for GGUF model files with 100% llama.cpp compatibility.

**Design Philosophy**:
- üîí **Correctness over performance** - Match llama.cpp exactly
- üì¶ **Minimal dependencies** - thiserror + regex only
- ü¶Ä **Pure Rust** - No C++ bindings required
- ‚úÖ **Validation-driven** - Every tokenizer validated against llama.cpp

---

## ‚úÖ v0.7.0 - Full llama.cpp Parity (Current)

### Tokenizer Coverage
| Type | Name | Status | Validated Against |
|------|------|--------|-------------------|
| SPM | SentencePiece | ‚úÖ | llama-spm |
| BPE | Byte-Pair Encoding | ‚úÖ | gpt-2, starcoder, qwen2, deepseek-coder, deepseek-llm, falcon, command-r, refact |
| WPM | Word-Piece Model | ‚úÖ | bert-bge |
| UGM | Unigram | ‚úÖ | *(implementation complete, needs T5 GGUF)* |
| RWKV | RWKV World | ‚úÖ | *(implementation complete, needs model)* |
| - | PLaMo-2 | ‚úÖ | *(no GGUF available from llama.cpp)* |

### Validated Models (10/10 passing)
All models validated against `llama-tokenize` binary with exact token match:

- ‚úÖ `ggml-vocab-bert-bge.gguf` (WPM)
- ‚úÖ `ggml-vocab-command-r.gguf` (BPE)
- ‚úÖ `ggml-vocab-deepseek-coder.gguf` (BPE)
- ‚úÖ `ggml-vocab-deepseek-llm.gguf` (BPE)
- ‚úÖ `ggml-vocab-falcon.gguf` (BPE)
- ‚úÖ `ggml-vocab-gpt-2.gguf` (BPE)
- ‚úÖ `ggml-vocab-llama-spm.gguf` (SPM)
- ‚úÖ `ggml-vocab-qwen2.gguf` (BPE)
- ‚úÖ `ggml-vocab-refact.gguf` (BPE)
- ‚úÖ `ggml-vocab-starcoder.gguf` (BPE)

### Public API (Stable)
```rust
// Core API
Tokenizer::from_gguf_file(path) -> Result<Tokenizer>
tokenizer.encode(text, add_special_tokens) -> Result<Vec<TokenId>>
tokenizer.decode(&token_ids) -> Result<String>
tokenizer.decode_single(token_id) -> Result<String>

// Metadata
tokenizer.vocab_size() -> usize
tokenizer.bos_token() -> Option<TokenId>
tokenizer.eos_token() -> Option<TokenId>
tokenizer.model_type() -> &str
tokenizer.pre_type() -> &str

// Batch processing
tokenizer.encode_batch(texts, add_special) -> Result<Vec<Vec<TokenId>>>
```

---

## üîÆ v0.8.0 - Extended Validation

**Target**: Additional tokenizer validation with available models

### Planned
- [ ] **RWKV validation** - Test with `rwkv-4-pile-169m` GGUF when available
- [ ] **T5/UGM validation** - Investigate llama.cpp T5 architecture support
- [ ] **Additional BPE patterns** - Any new vocab files from llama.cpp updates

### Stretch Goals
- [ ] **Phi-4 validation** - When GGUF available
- [ ] **Llama-3.1/3.2 validation** - Verify continued compatibility

---

## üåü Future Considerations

### May Implement
- **SIMD optimization** - Performance without sacrificing correctness
- **Streaming encoder** - Token-by-token encoding for very large texts
- **Async file loading** - Non-blocking GGUF parsing

### Will Not Implement
- ‚ùå C++ dependencies or FFI
- ‚ùå Training/fine-tuning support
- ‚ùå Model inference capabilities
- ‚ùå Tokenizer training from scratch
- ‚ùå Non-GGUF format support (safetensors, etc.)

---

## üìä Version History

| Version | Date | Highlights |
|---------|------|------------|
| v0.7.0 | Jan 2025 | Full llama.cpp parity: WPM, UGM, RWKV, PLaMo-2 |
| v0.6.0 | Jan 2025 | llama.cpp validation fixes |
| v0.4.0 | Oct 2024 | Streaming decode, token introspection |
| v0.3.0 | Oct 2024 | Mistral, Qwen, Gemma support |
| v0.2.0 | Oct 2024 | Batch encoding, benchmarks |
| v0.1.0 | Oct 2024 | Initial release: SPM + BPE |

---

## üîó Related Projects

- **[libshimmy](https://github.com/Michael-A-Kuykendall/libshimmy)** - Pure Rust LLM inference (uses shimmytok)
- **[llama.cpp](https://github.com/ggerganov/llama.cpp)** - Reference C++ implementation
- **[GGUF spec](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)** - File format documentation

---

**Maintainer**: Michael A. Kuykendall  
**License**: MIT  
**Last Updated**: January 2025
