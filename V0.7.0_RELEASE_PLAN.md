# shimmytok v0.7.0 Release Plan

**Target**: Full llama.cpp tokenizer parity  
**Estimate**: 69 Fibonacci points  
**Golden Source**: `../llama.cpp` (local clone)

---

## Phase 0: Golden Test Vector Generation (5 pts)

- [ ] 0.1 Verify llama.cpp exists at `../llama.cpp`
- [ ] 0.2 Build llama.cpp tokenize tool (if needed)
- [ ] 0.3 Generate golden vectors for SPM (Llama-3.2-1B)
- [ ] 0.4 Generate golden vectors for BPE (GPT-2)
- [ ] 0.5 Save fixtures to `tests/fixtures/golden_*.json`

---

## Phase 1: clean_spaces Implementation (3 pts)

- [x] 1.1 Add `apply_clean_spaces()` helper in lib.rs
- [x] 1.2 Wire into `decode_with_options()` when `vocab.clean_spaces()` is true
- [x] 1.3 Test: punctuation spacing ` ?` → `?`
- [x] 1.4 Test: apostrophe contractions ` 'm` → `'m`

---

## Phase 2: WPM Tokenizer (8 pts)

- [x] 2.1 Create `src/wpm.rs` with `WpmTokenizer` struct
- [x] 2.2 Implement `preprocess_wpm()` - NFD normalize, lowercase, split
- [x] 2.3 Implement `encode()` - phantom space + greedy longest match
- [x] 2.4 Implement `decode()` - concatenate token texts
- [x] 2.5 Add `WpmTokenizer` to lib.rs exports
- [ ] 2.6 Wire into `Tokenizer` dispatch for `model_type == "bert"`
- [ ] 2.7 Test: basic WPM encoding/decoding
- [ ] 2.8 Test: golden vector comparison (if WPM model available)

---

## Phase 3: RWKV Tokenizer (8 pts)

- [x] 3.1 Create `src/rwkv.rs` with `RwkvTokenizer` struct
- [x] 3.2 Implement `unescape_rwkv_token()` - `\n`, `\t`, `\xNN` handling
- [x] 3.3 Implement trie structure for token matching
- [x] 3.4 Implement `encode()` - greedy longest trie match
- [x] 3.5 Implement `decode()` - concatenate unescaped tokens
- [x] 3.6 Add `RwkvTokenizer` to lib.rs exports
- [ ] 3.7 Wire into `Tokenizer` dispatch for `model_type == "rwkv"`
- [ ] 3.8 Test: unescape function unit tests
- [ ] 3.9 Test: basic RWKV encoding

---

## Phase 4: UGM Tokenizer (13 pts)

- [x] 4.1 Create `src/ugm.rs` with `UgmTokenizer` struct
- [x] 4.2 Implement `NaiveTrie` structure (byte-level)
- [x] 4.3 Implement trie building from vocab (normal + user_defined + unused)
- [x] 4.4 Implement unknown token score calculation (min_score - 10.0)
- [x] 4.5 Implement `normalize_ugm()` stub (escape whitespace handling)
- [x] 4.6 Implement Viterbi DP forward pass
- [x] 4.7 Implement backtracking to extract tokens
- [x] 4.8 Implement `decode()` - handle escaped space `▁`
- [x] 4.9 Add `UgmTokenizer` to lib.rs exports
- [ ] 4.10 Wire into `Tokenizer` dispatch for `model_type == "t5"`
- [ ] 4.11 Test: trie construction
- [ ] 4.12 Test: basic UGM encoding
- [ ] 4.13 Test: unknown token fallback

---

## Phase 5: PLAMO2 Tokenizer (21 pts)

- [x] 5.1 Create `src/plamo2.rs` with `Plamo2Tokenizer` struct
- [x] 5.2 Define constants: INVALID_SCORE, UNKNOWN_SCORE, table columns
- [x] 5.3 Implement byte_token array building (256 `<0xNN>` tokens)
- [x] 5.4 Implement suffix_to_score collection from vocab
- [x] 5.5 Implement suffix sorting (by reversed string)
- [x] 5.6 Implement suffix_to_id and to_suffix_id mapping
- [x] 5.7 Build flattened table with [piece_len, token_id, score, piece_id]
- [x] 5.8 Implement `encode()` reverse DP (scores array)
- [x] 5.9 Implement path tracking during DP
- [x] 5.10 Implement suffix_id lookup during DP iteration
- [x] 5.11 Implement forward path decoding
- [x] 5.12 Implement byte fallback for unknown codepoints
- [x] 5.13 Implement `decode()` - reconstruct from byte tokens
- [x] 5.14 Add `Plamo2Tokenizer` to lib.rs exports
- [ ] 5.15 Wire into `Tokenizer` dispatch for `model_type == "plamo2"`
- [ ] 5.16 Test: byte token mapping
- [ ] 5.17 Test: suffix table construction
- [ ] 5.18 Test: DP path correctness
- [ ] 5.19 Test: byte fallback behavior
- [ ] 5.20 Test: round-trip encode/decode
- [ ] 5.21 Test: golden vector comparison (if PLaMo model available)

---

## Phase 6: Integration & Wiring (5 pts)

- [ ] 6.1 Update `Tokenizer::from_gguf_file()` to detect new model types
- [ ] 6.2 Add `TokenizerBackend` enum variant for each new type
- [ ] 6.3 Implement dispatch in `encode()` for all backends
- [ ] 6.4 Implement dispatch in `decode()` for all backends
- [ ] 6.5 Integration test: load real GGUF files for each type (skip if unavailable)

---

## Phase 7: Documentation & Release (6 pts)

- [x] 7.1 Update CHANGELOG.md with all v0.7.0 changes
- [x] 7.2 Update README.md supported model types table
- [x] 7.3 Bump version in Cargo.toml to 0.7.0
- [ ] 7.4 Run full test suite (`cargo test`)
- [ ] 7.5 Run clippy (`cargo clippy`)
- [ ] 7.6 Final commit with release message

---

## Summary

| Phase | Description | Points |
|-------|-------------|--------|
| 0 | Golden vectors | 5 |
| 1 | clean_spaces | 3 |
| 2 | WPM | 8 |
| 3 | RWKV | 8 |
| 4 | UGM | 13 |
| 5 | PLAMO2 | 21 |
| 6 | Integration | 5 |
| 7 | Docs & Release | 6 |
| **Total** | | **69** |

---

## Notes

- GPT_PLANS.md contains full Rust implementations for WPM, UGM, RWKV, PLAMO2
- llama.cpp at `../llama.cpp` is source of truth for golden vectors
- Skip model-specific tests if GGUF files unavailable (use `#[ignore]`)
- WPM in llama.cpp is NOT BERT WordPiece (`##`), it's phantom-space + greedy
